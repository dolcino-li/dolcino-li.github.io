# DynoYarn

> SLS Apache page: https://hadoop.apache.org/docs/stable/hadoop-sls/SchedulerLoadSimulator.html
> LinkedIn DynoYarn Introduction Page: https://engineering.linkedin.com/blog/2021/scaling-linkedin-s-hadoop-yarn-cluster-beyond-10-000-nodes
> DynoYarn Ticket: https://issues.apache.org/jira/browse/YARN-8849
> DynoYarn git repo: https://github.com/linkedin/dynoyarn

## Background

At the beginning of this year, the Resource Manager in our cluster met some performance issues. We found that applications in our cluster were running slowly, but there was still resource remaining. After that, we have done some optimization for our RM code and added some metrics to monitor RM to make it better.

However, since our cluster has over 10k nodes, we need to know when we will come across the same issue with the applications growing up. We need to do the simulation of RM with some pressure test.

## SLS vs DynoYarn
- SLS:
  - Scheduler Load Simulator, is such a tool, which can simulate large-scale YARN clusters and application loads in a single machine.  
- DynoYARN
  - DynoYARN is a tool to spin up on-demand YARN clusters and run simulated YARN workloads for scale testing

I have compared SLS with DynoYarn and choose DynoYarn as the tool to do the pressure test for sever reason.

- SLS can only simulate a scheduler. Other components in Resource Manager will also influence the performance. 
- SLS can only run in a single machine. Simulation of 10k nodes and 10k apps is impossible.
- Metrics of SLS don't match with real RM.
- DynoYarn could run in a real cluster with high scalability.
- DynoYarn could run a real resource manager.

## DynoYarn Structure
DynoYarn has two part / two applications.

For the first application named dynoyarn-driver, it will launch DynoYarn ResourceManager with lots of NodeManagers.

For the second application named dynoyarn-generator, it will generate workload to the DynoYarn cluster.

**Detail steps are as following.**

![DynoYarn](/assets/img/dynoyarn/DynoYarn.png)

1. Driver client submits to real cluster.
2. DriverApplicationMaster Launches and allocates a container to run Mesher.
3. First Mesher launch simulated ResourceManager (DynoYarn RM)
4. First Mesher will writes dyno yarn clusters’ port info to HDFS and reads scheduler info.
5. DriverApplicationMaster waits for simulated RM to launch.
Mesher launches simulated NodeManagers(DynoYarn NMs). Every container could launch lots of NodeManager instances. NMs could read RM’s info from HDFS, and then register to RMs. After that, dyno yarn cluster is ready to work.
6. Workload Client submits to real cluster.
7. Workload Application Master launches and allocates a container to run SimulatedAppSubmitter.
8. SimulatedAppSubmitter reads workload info and simulated cluster info from HDFS.
9. SimulatedAppSubmitter runs several SimulatedClient to submit to the DynoYarn cluster.
10. SimulatedClient submit to Simulated RM.
11. SimulatedApplicationMaster launches in DynoYarn NodeManager.
12. SimulatedApplicationMaster allocates containers from DynoYarn RMs and then launches those containers at another DynoYarn NodeManager. But those containers only run bash command “sleep” with a specific time.

**Customize DynoYarn cluster**

For each cluster, there will be different node partitions with different node labels. Normally, nodes with different node labels will have different CPU or Memory resources.

To simulate this situation, we need to use several yarn commands to update each node property.

Confs of yarn-site.xml for Client to reach the DynoYarn cluster.

- yarn.resourcemanager.admin.address
- yarn.resourcemanager.address

For node label:

- Add node label to cluster:
  - yarn rmadmin -addToClusterNodeLabels ${label}
- List All Nodes:
  - yarn node -list
- Update node label:
  - yarn rmadmin -replaceLabelsOnNode "\${nodes}=\${labels}"

For CPU & memory Resource:

- Update Node Resource:
  - yarn rmadmin -updateNodeResource \${nodes} \${memories} \${vCores}

**Resource DynoYarn Need**

In simulation, One container runs as RM, and then each container runs as 50 NMs. 

To submit to DynoYarn cluster, each simulatedAppSubmitter submits 10k apps to RM.

When RM runs with 10k NMs and 10k apps, RM needs roughly 40G memory.  When 50 NMs in one container, the container needs roughly 20G memory. When simulatedAppSubmitter submits 1k apps, the container needs roughly 20G memory.
Therefore, Resource DynoYarn Need is $$M_{RM}+M_{NM}\times\frac{\#NMs}{NMsPerContainer}+ M_{submitter} \times\frac{TotalJobs}{JobsPerContainer}$$ 
In our case, the resource is $$40G + 20G \times\frac{10k}{50} + 20G\times\frac{10k}{1k} = 4240G$$

The Cluster we simulate have roughly 5PB

**Setup Monitor Metrics**

To monitor metrics as our cluster does, We can get metrics through the website of DynoYarn RM.
Same as DynoYarn RM, ${RM_Host}:{RM_HTTP_PORT}/jmx  contains the metrics for the current cluster.
Then, Set up a specifical program to get the metrics, then all metrics could be recorded and checked in prometheus.

To make it easier to get metrics, we fixed some port.
RM_Port: 50032, RM_Admin_Port: 50033, RM_HTTP_Port: 50031

**The metrics used to measure the performance**

- NodeUpdateCountPerSecond: 
  - Number of NodeUpdateEvent that scheduler could handle per second. Normally, each NodeUpdateEvent could allocate a similar resource, therefore, the more NodeUpdateEvent handles, the more resource the RM could allocate to applications. [Cluster Level]
- ContainerAllocatedPerMinute:  
  - Number of containers allocated to applications per minute. Normally this kind of speed is stable, but if each node manager could have more resources(add memory or vcores), then each NodeUpdateEvent will allocate more resources one time. [Cluster Level]
- AppAttemptFirstContainerAllocationDelayAvgTime:  
  - Average time RM spends to allocate the first container(application master) for all attempts. If the application is waiting for a resource because of resource limit or schedule latency. This metric could reflect real waiting time for applications in each queue. [Queue Level]


## Workload Generate

**Workload Structure**

Following are the examples of workload json structure.

![WorkloadExample](/assets/img/dynoyarn/WorkloadExample.png)
- Basic Field:
  - appId: original appId.
  - submitTime: original submit time.
  - user: User name who submitted this job.
  - queue: Queue which this job submit to. 
  - nodeLabel: Special node this job needs to be scheduled.

- AMResourceRequest:
  - memoryMB: Memory resource Am needs.
  - Vcores: CPU resource Am needs.

- ResourceRequestSpecs: This field is a list. For each element in list:
  - numInstances: total containers.
  - priority:  resource request priority when allocating containers.
  - resource: resource each container needs.
  - runTimes: A list of run times for each container.
  - averageRunTime: average running time for all containers.
    - If you set up averagetRunTime, then runTimes will not work.
    - If you set up runTimes, the length of runTimes list needs to be equal to numInstances.

**The Way Workload App runs**

![WorkloadFlow](/assets/img/dynoyarn/WorkloadFlow.png)

From the diagram above, when submitting an application to the dyno yarn cluster, a simulated client will first allocate a container to be ApplicationMaster, the resource cost is 16384 MB with 2 vcores. And then, the simulated application master will allocate containers by resource request specs. SimulatedAm will first sort every resource request by priority, and then allocate containers with required resources and numbers.

For request B，it has high priority, therefore, B requests run first. By using runTimes, each container will have different run times. When all containers finish, then SimulatedAm will run another request.

Next request is A, it uses averageRunTimes, therefore, every container runs 30s.

After all resource requests finish, this app finishes as well.

**Generate Workload from audit log**

To simulate the workload on our cluster, we wrote some script to get container runtime and used resources from RM logs. Scan every line in RM logs, and use regex to get info we need.

- Get Application Start Info
  - Regex ApplicationStartPattern = ``"(.*) INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application (.*) from user: (.*) activated in queue: (.*)"``
  - Regex return:  ``('2022-07-01 00:51:38,871', 'application_1655861294850_748940', 'b_test', 'testQueue')``

- Get Container Start Info
  - Regex ContainerStartPattern = ``"(.*) INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container (.*) of capacity <memory:(.*), vCores:(.*)> on host *"``
- Regex return:  ``('2022-07-01 00:51:38,874', 'container_e3786_1655861294850_748940_01_000001', '4096', '1')``

- Get Container End Info
  - Regex ContainerStopPattern = ``"(.*) INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container (.*) of capacity <memory:(.*), vCores:(.*)> on host *"``
  - Regex return:  ``('2022-07-01 00:51:38,874', 'container_e3786_1655861294850_748940_01_000001', '4096', '1')``



- ApplicationStartPattern:
  - Get SubmitTime / AppId / User name / queue name
  - After cluster start, the application will wait (submitTime - clusterStartTime) 
  - clusterStartTime is set on dynoyarn.xml configuration.
- ContainerStartPattern:
  - Get containerId, resource(Memory, vCores), container start time.
  - If containerId is 01 or 02, then this container should be Application Master.
- ContainerStopPattern:
  - Get containerId, resource(Memory, vCores), container stop time.
  - Calculate container runTime by (endTime - startTime).
- Exceptions:
  - If a container doesn’t have containerStopPattern, then container runTime is (TimeNow - startTime)
  - If an app doesn’t have container whose Id is 01 or 02, then use a default resource to run as application master and runtime is (LastContainerEndTime - clusterStartTime)

By this way, workload could be generated.


**Difference between generated workload and real workload**

![Diff](/assets/img/dynoyarn/Diff.png)

In the above section, we have discussed how workload is generated and running in the dyno yarn cluster. 

However, in our cluster, 90% of jobs are spark jobs, each spark job will have a DAG, which means some resource requests are dependent on others. Meanwhile spark has dynamic allocation perproty, which means spark could allocate other containers to join the running stages. 

In the workload we generated, we only have linear resource requests, and we can’t simulate dynamic allocation. (We also can’t get this info from the log).

Another perspective is that some applications may have pending resource requests, but can’t get containers because of resource limitation. For this situation, RM will need time to calculate the limitation. But the generated workload can’t get this kind of info, can’t simulate as well.

We can still use this workload as a benchmark, since the number of applications for each queue is the same as our cluster.

## Problems.

**Web Proxy**

![webproxy](/assets/img/dynoyarn/webproxy.png)

unset application web proxy base to avoid the /poxy/${appid} for RM

**Security between DynoYarn container and HDFS**

Make driver app place simulatedFatJar for generator app.

Original plan is that generator app need to upload simulatedFatJat to HDFS, and then download the simulatdFatJar from HDFS when it runs simulated container.
However, dyno yarn cluster is unsecurity, the cluster use simple auth. Therefore, the simulated container which use simple auth can never download the file from HDFS which use kerberos auth.

When driver app runs, it could download all necessary files, and put the file in a centain path. Meanwhile, generator app add that path into CLASSPATH. Then we can avoid above situation.

My implement about those problems.
- https://github.com/linkedin/dynoyarn/pull/3